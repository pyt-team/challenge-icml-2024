{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from modules.data.load.loaders import PointCloudLoader\n",
    "from modules.data.preprocess.preprocessor import PreProcessor\n",
    "from modules.utils.utils import (\n",
    "    # describe_data,\n",
    "    load_dataset_config,\n",
    "    load_model_config,\n",
    "    load_transform_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset configuration for P12:\n",
      "\n",
      "{'data_dir': 'datasets/pointcloud/P12data',\n",
      " 'data_file': 'processed_data/PTdict_list.npy',\n",
      " 'label_file': 'processed_data/arr_outcomes.npy',\n",
      " 'num_features': 6,\n",
      " 'num_classes': 2}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"P12\"\n",
    "dataset_config = load_dataset_config(dataset_name)\n",
    "loader = PointCloudLoader(dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transform configuration for pointcloud2simplicial/raindrop_drop_top_lifting:\n",
      "\n",
      "{'transform_type': 'lifting',\n",
      " 'transform_name': 'RaindropDropTopLifting',\n",
      " 'n_sens': 4,\n",
      " 'd_model': 8,\n",
      " 'n_layers': 2,\n",
      " 'n_heads': 1,\n",
      " 'drop_ratio': 0.4,\n",
      " 'n_classes': 2,\n",
      " 'd_static': 9,\n",
      " 'max_len': 215,\n",
      " 'threshold': 0.2,\n",
      " 'epoch': 1}\n"
     ]
    }
   ],
   "source": [
    "# Define transformation type and id\n",
    "transform_type = \"liftings\"\n",
    "# If the transform is a topological lifting, it should include both the type of the lifting and the identifier\n",
    "transform_id = \"pointcloud2simplicial/raindrop_drop_top_lifting\"\n",
    "\n",
    "# Read yaml file\n",
    "transform_config = {\n",
    "    # \"preprocess_data\": preprocess_data(),\n",
    "    \"lifting\": load_transform_config(transform_type, transform_id),\n",
    "    # other transforms (e.g. data manipulations, feature liftings) can be added here\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform parameters are the same, using existing data_dir: datasets/pointcloud/P12data/lifting/1744592691\n"
     ]
    }
   ],
   "source": [
    "lifted_dataset = PreProcessor(dataset, transform_config, loader.parameters.data_dir)\n",
    "# describe_data(lifted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[215, 11988, 4], y=[11988], time=[215, 11988, 1], static=[11988, 9], incidence_0=[1, 4], down_laplacian_0=[4, 4], up_laplacian_0=[4, 4], adjacency_0=[4, 4], hodge_laplacian_0=[4, 4], incidence_1=[4, 6], down_laplacian_1=[6, 6], up_laplacian_1=[6, 6], adjacency_1=[6, 6], hodge_laplacian_1=[6, 6], incidence_2=[6, 4], down_laplacian_2=[4, 4], up_laplacian_2=[4, 4], adjacency_2=[4, 4], hodge_laplacian_2=[4, 4], shape=[3], x_0=[4, 4], x_1=[6], eho='bite', x_2=[4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifted_dataset.get(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model configuration for simplicial SAN:\n",
      "\n",
      "{'in_channels': None,\n",
      " 'hidden_channels': 32,\n",
      " 'out_channels': None,\n",
      " 'n_layers': 2,\n",
      " 'n_filters': 2,\n",
      " 'order_harmonic': 5,\n",
      " 'epsilon_harmonic': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from modules.models.simplicial.san import SANModel\n",
    "\n",
    "model_type = \"simplicial\"\n",
    "model_id = \"san\"\n",
    "model_config = load_model_config(model_type, model_id)\n",
    "\n",
    "model = SANModel(model_config, dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlifted_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/envs/icml/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Money/challenge-icml-2024/modules/models/simplicial/san.py:47\u001b[0m, in \u001b[0;36mSANModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass of the model.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m        Output tensor.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_laplacian_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_laplacian_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/envs/icml/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/envs/icml/lib/python3.11/site-packages/topomodelx/nn/simplicial/san.py:123\u001b[0m, in \u001b[0;36mSAN.forward\u001b[0;34m(self, x, laplacian_up, laplacian_down)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Forward computation\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 123\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlaplacian_up\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlaplacian_down\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojection_mat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/envs/icml/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/envs/icml/lib/python3.11/site-packages/topomodelx/nn/simplicial/san_layer.py:212\u001b[0m, in \u001b[0;36mSANLayer.forward\u001b[0;34m(self, x, laplacian_up, laplacian_down, projection_mat)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass of the SAN Layer.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m.. math::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    Output tensor.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Compute the down and up convolutions\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m z_down \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_down\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlaplacian_down\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m z_up \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_up(x, laplacian_up)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# For the harmonic convolution, we use the precomputed projection matrix P as the neighborhood\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# with no attention\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/envs/icml/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/envs/icml/lib/python3.11/site-packages/topomodelx/nn/simplicial/san_layer.py:101\u001b[0m, in \u001b[0;36mSANConv.forward\u001b[0;34m(self, x_source, neighborhood)\u001b[0m\n\u001b[1;32m     98\u001b[0m x_message \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(x_source, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Reshape required to re-use the attention function of parent Conv class\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# -> [num_nodes, out_channels * n_filters]\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m x_message_reshaped \u001b[38;5;241m=\u001b[39m \u001b[43mx_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_filters\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# SAN always requires attention\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# In SAN, neighborhood is defined by lower/upper laplacians; we only use them as masks\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# to keep only the relevant attention coeffs\u001b[39;00m\n\u001b[1;32m    108\u001b[0m neighborhood \u001b[38;5;241m=\u001b[39m neighborhood\u001b[38;5;241m.\u001b[39mcoalesce()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "# y_hat = model(lifted_dataset.get(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
