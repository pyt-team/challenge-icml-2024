{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PointCloud to Graph Protein Lifting Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "This notebook shows how to import UniProt protein data and convert it to a graph using the `PointCloudToGraph` class. Proteins are represented as point clouds where each point is a residue in the protein, setting CarbonAlpha as its centers. The graph is created by connecting residues that are close to each other in the 3D space or that appear in a sequential order.\n",
    "\n",
    "The target is the mass of each protein.\n",
    "\n",
    "The notebook is divided into sections:\n",
    "\n",
    "- [Loading the dataset](#loading-the-dataset) loads the config files for the data and the desired tranformation, creates a dataset object and visualizes it.\n",
    "- [Loading and applying the lifting](#loading-and-applying-the-lifting) definding the edges by the following way:\n",
    "    - **Sequentialwise**: Connecting residues that appear in a sequential order (one after another). This approach is based on the presence of peptide bonds, which link the amino acids in a protein chain in a specific sequence.\n",
    "    - **KNN**: Connecting residues that are close to each other in the 3D space. This approach is based on the physical proximity of the residues in the protein structure.\n",
    "- [Create and run a simplicial nn model](#create-and-run-a-simplicial-nn-model) simply runs a forward pass of the model to check that everything is working as expected.\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "Note that for simplicity the notebook is setup to use a point cloud. \n",
    "\n",
    "With this submission, **UniProt** protein dataset is available and loaded as a point cloud, based on PDB files.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# With this cell any imported module is reloaded before each cell execution\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from modules.data.load.loaders import PointCloudLoader\n",
    "from modules.data.preprocess.preprocessor import PreProcessor\n",
    "from modules.utils.utils import (\n",
    "    describe_data,\n",
    "    load_dataset_config,\n",
    "    load_model_config,\n",
    "    load_transform_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just need to specify the name of the available dataset that we want to load. First, the dataset config is read from the corresponding yaml file (located at `/configs/datasets/` directory), and then the data is loaded via the implemented `Loaders`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset configuration for UniProt:\n",
      "\n",
      "{'data_domain': 'pointcloud',\n",
      " 'data_type': 'UniProt',\n",
      " 'data_name': 'UniProt',\n",
      " 'data_dir': 'datasets/pointcloud/UniProt',\n",
      " 'query': 'length:[95 TO 155]',\n",
      " 'format': 'tsv',\n",
      " 'fields': 'accession,length',\n",
      " 'size': 20,\n",
      " 'num_features': 20,\n",
      " 'num_classes': 1,\n",
      " 'task': 'regression',\n",
      " 'loss_type': 'mse',\n",
      " 'monitor_metric': 'mae',\n",
      " 'task_level': 'graph'}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"UniProt\"\n",
    "dataset_config = load_dataset_config(dataset_name)\n",
    "loader = PointCloudLoader(dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset configuration for UniProt:\n",
      "\n",
      "{'data_domain': 'pointcloud',\n",
      " 'data_type': 'UniProt',\n",
      " 'data_name': 'UniProt',\n",
      " 'data_dir': 'datasets/pointcloud/UniProt',\n",
      " 'query': 'length:[95 TO 155]',\n",
      " 'format': 'tsv',\n",
      " 'fields': 'accession,length',\n",
      " 'size': 20,\n",
      " 'num_features': 20,\n",
      " 'num_classes': 1,\n",
      " 'task': 'regression',\n",
      " 'loss_type': 'mse',\n",
      " 'monitor_metric': 'mae',\n",
      " 'task_level': 'graph'}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"UniProt\"\n",
    "dataset_config = load_dataset_config(dataset_name)\n",
    "loader = PointCloudLoader(dataset_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then access to the data through the `load()`method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDB file for O14960 already exists.\n",
      "PDB file for O14907 already exists.\n",
      "PDB file for O14519 already exists.\n",
      "PDB file for O60519 already exists.\n",
      "PDB file for O75379 already exists.\n",
      "PDB file for A6NNB3 already exists.\n",
      "PDB file for O60814 already exists.\n",
      "PDB file for C9JLW8 already exists.\n",
      "PDB file for O43914 already exists.\n",
      "PDB file for A2RU14 already exists.\n",
      "PDB file for O75956 already exists.\n",
      "PDB file for O15116 already exists.\n",
      "PDB file for O14933 already exists.\n",
      "PDB file for O00453 already exists.\n",
      "PDB file for A6NFY7 already exists.\n",
      "PDB file for O00422 already exists.\n",
      "PDB file for O15540 already exists.\n",
      "PDB file for O15511 already exists.\n",
      "PDB file for O95139 already exists.\n",
      "PDB file for A8MQ03 already exists.\n",
      "PDB file for O14960 already exists.\n",
      "PDB file for O14907 already exists.\n",
      "PDB file for O14519 already exists.\n",
      "PDB file for O60519 already exists.\n",
      "PDB file for O75379 already exists.\n",
      "PDB file for A6NNB3 already exists.\n",
      "PDB file for O60814 already exists.\n",
      "PDB file for C9JLW8 already exists.\n",
      "PDB file for O43914 already exists.\n",
      "PDB file for A2RU14 already exists.\n",
      "PDB file for O75956 already exists.\n",
      "PDB file for O15116 already exists.\n",
      "PDB file for O14933 already exists.\n",
      "PDB file for O00453 already exists.\n",
      "PDB file for A6NFY7 already exists.\n",
      "PDB file for O00422 already exists.\n",
      "PDB file for O15540 already exists.\n",
      "PDB file for O15511 already exists.\n",
      "PDB file for O95139 already exists.\n",
      "PDB file for A8MQ03 already exists.\n"
     ]
    }
   ],
   "source": [
    "dataset = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Applying the Lifting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will instantiate the lifting we want to apply to the data. For this example the knn lifting was chosen. The algorithm takes the k nearest neighbors for each node and creates a hyperedge with them. Moreover, the algorithm also creates an edge for each sequential pair of residues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transform configuration for pointcloud2graph/knn_lifting:\n",
      "\n",
      "{'transform_type': 'lifting',\n",
      " 'transform_name': 'PointCloudKNNLifting',\n",
      " 'max_cell_length': None,\n",
      " 'preserve_edge_attr': False,\n",
      " 'feature_lifting': 'ProjectionSum',\n",
      " 'k_value': 10,\n",
      " 'loop': False}\n"
     ]
    }
   ],
   "source": [
    "# Define transformation type and id\n",
    "transform_type = \"liftings\"\n",
    "# If the transform is a topological lifting, it should include both the type of the lifting and the identifier\n",
    "transform_id = \"pointcloud2graph/knn_lifting\"\n",
    "\n",
    "# Read yaml file\n",
    "transform_config = {\n",
    "    \"lifting\": load_transform_config(transform_type, transform_id)\n",
    "    # other transforms (e.g. data manipulations, feature liftings) can be added here\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We than apply the transform via our `PreProcesor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform parameters are the same, using existing data_dir: /home/bmiquel/Documents/Projects/Topo/challenge-icml-2024/datasets/pointcloud/UniProt/UniProt/lifting/1540663474\n",
      "\n",
      "Dataset contains 20 samples.\n",
      "\n",
      "Providing more details about sample 0/20:\n",
      " - Graph with 151 vertices and 1730 edges.\n",
      " - Features dimensions: [20, 0]\n",
      " - There are 0 isolated nodes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lifted_dataset = PreProcessor(dataset, transform_config, loader.data_dir)\n",
    "describe_data(lifted_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run a Cell NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section a simple model is created to test that the used lifting works as intended. A graph neural network from torch_geometric is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model configuration for graph GRAPHSAGE:\n",
      "\n",
      "{'in_channels_0': None,\n",
      " 'in_channels_1': None,\n",
      " 'in_channels_2': None,\n",
      " 'hidden_channels': 32,\n",
      " 'out_channels': None,\n",
      " 'n_layers': 2}\n"
     ]
    }
   ],
   "source": [
    "from modules.models.graph.graphsage import GraphSAGEModel\n",
    "\n",
    "model_type = \"graph\"\n",
    "model_id = \"graphsage\"\n",
    "model_config = load_model_config(model_type, model_id)\n",
    "\n",
    "model = GraphSAGEModel(model_config, dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmiquel/Documents/Projects/Topo/challenge-icml-2024/tutorials/pointcloud2graph/../../modules/models/graph/graphsage.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.nn.functional.softmax(global_mean_pool(z, None))\n"
     ]
    }
   ],
   "source": [
    "y_hat = model(lifted_dataset.get(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct the cell above should execute without errors. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_topox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
